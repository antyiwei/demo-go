## 2018-04-13 golang 大会培训


## 1. 下载资料



## 2.上午第一段


## 3.上午第二段

### arrary,map 等 资料如下链接：
[gotraining/README.md at master · ardanlabs/gotraining · GitHub](https://github.com/ardanlabs/gotraining/blob/master/topics/go/language/arrays/README.md)


### 讲解
-- 测试nps 进入包后
go test -run none -bench . -benchtime 3s 
![0.pd6tcgvb8jb](images/0.pd6tcgvb8jb)


如果不把结果赋值给a，编译器会跳过


benchmark的时候cpu要闲着


benchmark的时候机器不要做别的事情

逐行扫描最快，链表次之，最差是按列扫描

主要是展示了处理器不同层级缓存的速度


[gotraining/README.md at master · ardanlabs/gotraining · GitHub](https://github.com/ardanlabs/gotraining/blob/master/topics/go/language/arrays/README.md)


这个是Intel 以前的一个模型，但是到现在硬件变化不大

![0.83kdkycf0zb](images/0.83kdkycf0zb)
![0.5z4079ui8pj](images/0.5z4079ui8pj)
这个是Intel 以前的一个模型，但是到现在硬件变化不大

如果速度重要你要关注只用L1,L2, L3 里面的数据


cpu太快会很热

他比较缓存的交互延迟


硬件也利用软件管理获取数据


每次从内存读数据会读取64b的数据,prefetcher会优化，但是我们的程序对内存的访问要可预测

我们写软件的时候需要写帮助处理器＂预取＂


array的遍历就是可预测的access pattern

jvm会把数据在内存中尽量放在一起
还有TLB 缓存

讲解说明：
[TLB与cache的深入分析 - CSDN博客](https://blog.csdn.net/lcw_202/article/details/5994406)

操作系统内容|内存管理模块

分页内存管理

TLB cache miss的话可能需要扫描整个内存


他的测试中，按列访问数据都不在同一个page中，linked list大部分还在一个page里

如果矩阵太大，第一行和第二行的数据缓存里不在一张页上。

<font color="red">尽量用slice，但是可读性更重要
</ont>


map会持续的把内存压缩在一起


下午回看一些array和slice的代码，讲value and pointer, 并发，profiling 和tracing


## 下午第一节

这部分是讲操作系统的线程调度的

一个cpu同一时间只能跑一个线程

他现在开始谈调度器和线程的关系。调度器负责于让线程利用处理器。

启动线程以后，处理器要获取数据然后放在缓存


一组线程可能会用到的数据有很大的几率已经存在于L1 cache中

有两种工作类型，处理器的还有IO

处理器可以一直工作，IO需要等

如果有很多IO工作，很多线程有用

cpu类型的任务，多线程可能不会有很大帮助，甚至反作用


写多线软件需要利用线程池

线程池限制线程的数量

一个task在变成wait状态之后，其他的task会被执行，当db请求返回之后，会重新变成runnable的状态，这样就有22个任务都在runnable状态

以他的经验来看，每个cpu3个线程，表现最佳

太少的话，有很多idle时间，太多的话，context swtich会浪费资源

我们可以通过增加处理器的方式来支持更多的线程，但是越往后，增加处理器带来的收益越小

要理解同步与协调

Data race 是如果两个线程利用内存同一个数据。一个线程在写，一个在读，所以我们应该为了避免问题负责同步。

go里也有global run queue 和local run queue

Go routine 在go 里面像线程

差别在context switch 从一个go routine 到另外一个比OS 线程快的多

（context switch 在go里非常快，因为go知道每个go routine要干什么）

问：
goroutine 有那些使用的局限性么 需要注意的地方 优势是快 有什么劣势么

利用Preemtive schedular 不能预测什么时候有context switch

4种事件 go, gc, systemcall, block

systemcall 现在大部分系统可以异步调用

Poller 负责等到一个go routine 异步任务做完之后然后把它放回global run queue

go里面每个处理器会有一个线程，go的调度器在线程将变成wait状态的时候，会把该线程detach，然后创建一个新线程执行其他任务

从作业系统角度来看，go的线程不进入wait 状态

go routine的调度器使系统级别的context switch最少化

一个进程就可以看成一个go routine池

问：实际上是N个os threads，M个goroutines，形成一个N:M scheduler。我理解是这样，不知道是不是正确

资料发现：
我跟别人在开发一个基于proof of capacity 的加密货币叫burst coin. 不像bitcoin 一样浪费电。https://github.com/PoC-Consortium

## 下午第二段

不要把chan看成一个queue而是看成 信号signaling


大部分时间我们需要一个保证，保证signal有接收方，并且在有一个接受者之前一直被阻塞

无缓冲的channel 不能保证接收方得到了channel 里的数据。


receive是发生在send之前的
保证带来的是不确定的延迟，被阻塞

根据我们的需要选择是要buffered还是unbuffered

在做 无阻塞队列处理机制的时候 先把任务插入到一个队列，用 chan来触发协程的处理循环  使用select 的default来实现携程有任务正在处理的话避免阻塞  携程的处理循环只有在任务队列为空的时候会等待chan 这种处理方式是否靠谱

队列实际上是个list

make channel 时不加数字就能得到一个无缓冲channel

不能用Print知道什么任务先发现了。

小意外：线掉了
![0.ak6wnmj3tgh](images/0.ak6wnmj3tgh)
fanout 和drop模式可以用这种类型
![0.9q89b95e2ho](images/0.9q89b95e2ho)
这种模式用在取消和deadline


![0.6lwwxpcijs8](images/0.6lwwxpcijs8)


在web request的场景不要使用bufferred chan
不要使用fanout模式

问：为什么？
答：可伸缩性很差

个人理解：等性能问题需要考虑（压力）

backpressure

超过接受能力就会有backpressure

这个模式可以处理超时

chan通常来讲会比较慢


这个场景是说所有的go routine都在同一时间写日志

项目为需要跟大量顾客提供视频。突然所有的go routine 发视频障碍了。


他想设计一个避免障碍的logger.
设想：
把写入日志的go routine独立出来

这里他要用到之前提到的drop 模式

你要确定所有的go routine 什么时候结束。

我们要多学习这些模式。


## 下午第三段
![0.vu2v7prmw1](images/0.vu2v7prmw1)


这段就是在一堆doc中找topic出现的次数

![0.aubfs2dvxv](images/0.aubfs2dvxv)


time。/trace > t.out
time 是用来测试程序运行耗时的工具

他想分析瓶颈在哪儿

![0.dj6thy8ag5v](images/0.dj6thy8ag5v)
第一个柱子叫flat 第二个叫commutative
|第一个告诉你函数用多少处理器时间

第二个告诉你函数和此里面的函数加一起利用多长时间

Gc 用很多处理器时间

一共进行了4次

Gc 导致到很多context switches

从分析结果可以看出来gc 利用6内核，但是我们的软件只利用一个。

![0.ne5zhpqj8qs](images/0.ne5zhpqj8qs)



使用多核处理，如下：
![0.da9ecu1hanl](images/0.da9ecu1hanl)


每个线程所在的CPU都会有一份found变量存在L1 cache中

当有一个线程改了found的值之后，所有的cpu都需要从内存中把found的新值取出来


这就是为什么有时候增加处理器数量会变得更糟

为了克服这个问题可以利用local 变量


少用atomic 如果有很多线程好处大。
问：
怎么理解？
能简要说一下用local的原理是什么吗[捂脸]？

答：为了减少用atomic
多次更改变为减少为一次


上述所用：
![0.gbwjd5ggens](images/0.gbwjd5ggens)

解释：
查找文件中 counter 原来是找到就要同步一次，现在是一个 routine 跑完以后把本次 routine 中的总数一次性更新到总的 counter 中，如果原来每篇文章找到10个 hit ，就从10次变成1次

在gc的时候如果遇到新的内存申请，会先不执行malloc，先gc，gc完了再malloc

这个软件让你从BBC 等等下载news feeds


hey工具可以用来产生负载

为了分析他推荐用ngrok

GODEBUG=gctrace=1 ./project > /dev/null
有图解。。。

问：
意思就是原子操作每次都要sync是吧？而非原子操作core不需要sync,是吗？？


第一个和第三个数字是stop the world time

不用作业系统找到memory leak, 利用go 的分析方式。

如果里面关于内存的数字一直增加你软件里就存在memory leak

[GitHub - rakyll/hey: HTTP load generator, ApacheBench (ab) replacement, formerly known as rakyll/boom](https://github.com/rakyll/hey)

他想给正在运行的web service生成一个trace的快照

如果服务运行在防火墙之后，可以绑定http pprof，可以随时查看服务的trace

命令：
go tool pprof -alloc_space http://localhost:5000/debug/pprof/heap

top 40 -cum 告诉你在哪儿有最多allocation

scvg把内存还给操作系统


减少gc 数量影响严重。用go 分析功能不要猜猜瓶颈在哪儿，你只要看分析结果。


gopher评价：
查死锁内存泄漏利器...
好用的东西太多，自己需要多学学哇。。。


![0.33x2k5ielgt](images/0.33x2k5ielgt)


![0.thd5ep9i15](images/0.thd5ep9i15)

![0.jl93pe2vw2k](images/0.jl93pe2vw2k)

![0.m9vbdxq1bs](images/0.m9vbdxq1bs)

如果你函数里面不用别的函数compiler 可以inline
避免allocation

问：
如果你函数里面不用别的函数compiler 可以inline  怎么理解？

leaf function就是说所有的逻辑在当前函数完成，不会调用其他函数，比如他这里被inline的NewBuffer函数只是初始化一个Buffer返回， 没有任何其他函数调用
就是，如果你一个函数内的代码很少而且不调用别的函数就可以被inline


错误日志分析

性能提升有3种，1. 算法，2. 减少allocation，3， 如何高效的读取数据


讲师的邮箱：
bill@ardanlabs.com







